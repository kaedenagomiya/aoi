{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c09db4bb-2585-4231-8fbf-d74637b74d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import IPython.display as ipd\n",
    "import pprint\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from librosa.filters import mel as librosa_mel_fn\n",
    "#import matplotlib\n",
    "#matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "\n",
    "import toybox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "afb2b595-e3f6-49ff-8ed7-bc0198d739ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_audio(audio, samplerate, title='time-domain waveform'):\n",
    "    \"\"\"\n",
    "    usage:\n",
    "        # audio is [channel, time(num_frames)] ex.torch.Size([1, 68608])\n",
    "        # audio[0,:]: list of 1ch audio data\n",
    "        # audio.shape[1]: int value of 1ch audio data length\n",
    "        audio, sample_rate = torchaudio.load(str(iwav_path))\n",
    "        %matplotlib inline\n",
    "        plot_audio(audio, sample_rate)\n",
    "    \"\"\"\n",
    "    # transform to mono\n",
    "    channel = 0\n",
    "    audio = audio[channel,:].view(1,-1)\n",
    "    # to numpy\n",
    "    audio = audio.to('cpu').detach().numpy().copy()\n",
    "    time = np.linspace(0., audio.shape[1]/samplerate, audio.shape[1])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12,9))\n",
    "    \n",
    "    ax.plot(time, audio[0, :])\n",
    "    ax.set_title(title, fontsize=20, y=-0.12)\n",
    "    ax.tick_params(direction='in')\n",
    "    #ax.set_xlim(0, 3)\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Amp')\n",
    "    #ax.legend()\n",
    "    plt.tight_layout()\n",
    "    fig.canvas.draw()\n",
    "    plt.show()\n",
    "    #fig.savefig('figure.png')\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "def plot_mel(tensors:list, titles:list[str]):\n",
    "    \"\"\"\n",
    "    usage:\n",
    "        mel = mel_process(...)\n",
    "        fig_mel = plot_mel([mel_groundtruth[0], mel_prediction[0]],\n",
    "                            ['groundtruth', 'inferenced(model)'])\n",
    "\n",
    "    \"\"\"\n",
    "    xlim = max([t.shape[1] for t in tensors])\n",
    "    fig, axs = plt.subplots(nrows=len(tensors),\n",
    "                            ncols=1,\n",
    "                            figsize=(12, 9),\n",
    "                            constrained_layout=True)\n",
    "\n",
    "    if len(tensors) == 1:\n",
    "        axs = [axs]\n",
    "    \n",
    "    for i in range(len(tensors)):\n",
    "        im = axs[i].imshow(tensors[i],\n",
    "                           aspect=\"auto\",\n",
    "                           origin=\"lower\",\n",
    "                           interpolation='none')\n",
    "        #plt.colorbar(im, ax=axs[i])\n",
    "        fig.colorbar(im, ax=axs[i])\n",
    "        axs[i].set_title(titles[i])\n",
    "        axs[i].set_xlim([0, xlim])\n",
    "    fig.canvas.draw()\n",
    "    #plt.show()\n",
    "    #plt.close()\n",
    "    plt.close(fig)  # fig.close() \n",
    "    return fig\n",
    "\n",
    "def convert_phn_to_id(phonemes, phn2id):\n",
    "    \"\"\"\n",
    "    phonemes: phonemes separated by ' '\n",
    "    phn2id: phn2id dict\n",
    "    \"\"\"\n",
    "    return [phn2id[x] for x in ['<bos>'] + phonemes.split(' ') + ['<eos>']]\n",
    "\n",
    "\n",
    "def text2phnid(text, phn2id, language='en', add_blank=True):\n",
    "    if language == 'en':\n",
    "        from text import G2pEn\n",
    "        word2phn = G2pEn()\n",
    "        phonemes = word2phn(text)\n",
    "        if add_blank:\n",
    "            phonemes = ' <blank> '.join(phonemes)\n",
    "        return phonemes, convert_phn_to_id(phonemes, phn2id)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Language should be en (for English)!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9535bef7-9bff-47af-a643-101b29fea30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_index = 3  # 0:gradtts, 1:gradseptts, 2:gradtfktts, 3:gradtfk5tts, 4:gradtimektts, 5:gradfreqktts\n",
    "ckpt_name = '820_651082.pt'\n",
    "# for inference\n",
    "#N_STEP = 100\n",
    "#TEMP = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "89da4b78-f5da-4e6f-ade4-82b19386fa18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config_path4model: configs/config_tfk_k5.yaml\n",
      "exists: True\n",
      "Exists configs/test_dataset.json\n",
      "loaded configs/test_dataset.json\n",
      "ckpt_path: logs4model/gradtfk5tts/run_tfk_k5/ckpt/gradtfk5tts_820_651082.pt\n",
      "ckpt_dir_exist :True\n",
      "ckpt_path_exist:True\n"
     ]
    }
   ],
   "source": [
    "# First, please check changing <model_name>\n",
    "#ckpt_file_dir: logs4model/<model_name>/<runtime_name>/ckpt/\n",
    "config_yaml = 'configs/config_exp_mid.yaml'\n",
    "config = toybox.load_yaml_and_expand_var('configs/config_exp_mid.yaml')\n",
    "\n",
    "model_info = [\n",
    "    ['gradtts', 'gt_k3'],\n",
    "    ['gradseptts', 'sgt_k3'],\n",
    "    ['gradtfktts', 'tfk_k3'],\n",
    "    ['gradtfk5tts', 'tfk_k5'],\n",
    "    ['gradtimektts', 'timek_k3'],\n",
    "    ['gradfreqktts', 'freqk_k3'],\n",
    "]\n",
    "model_name = f'{model_info[model_index][0]}' # gradtts, gradseptts, gradtfktts, gradtfk5tts, gradtimektts, gradfreqktts\n",
    "runtime_name = f'{model_info[model_index][1]}'\n",
    "\n",
    "config_path4model = Path(f'./configs/config_{runtime_name}.yaml')\n",
    "config4model = toybox.load_yaml_and_expand_var(config_path4model)\n",
    "print(f'config_path4model: {config_path4model}')\n",
    "print(f'exists: {config_path4model.exists()}')\n",
    "\n",
    "test_ds_path = Path(config['test_datalist_path'])\n",
    "if test_ds_path.exists():\n",
    "    print(f'Exists {str(test_ds_path)}')\n",
    "    with open(config['test_datalist_path']) as j:\n",
    "        test_ds_list = json.load(j)\n",
    "    print(f'loaded {test_ds_path}')\n",
    "else:\n",
    "    print(f'No exist {test_ds_path}')\n",
    "\n",
    "ckpt_dir_path = Path(f'./logs4model/{model_name}/run_{runtime_name}/ckpt')\n",
    "ckpt_path = ckpt_dir_path / f'{model_name}_{ckpt_name}'\n",
    "print(f\"ckpt_path: {ckpt_path}\")\n",
    "print(f'ckpt_dir_exist :{ckpt_dir_path.exists()}')\n",
    "print(f'ckpt_path_exist:{ckpt_path.exists()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b7c61835-59c4-467c-ab9b-6d7e0b18634d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 1024 22050 256 1024 0 8000 1234\n",
      "phn2id_path: ./configs/phn2id.json\n"
     ]
    }
   ],
   "source": [
    "# for audio params\n",
    "n_mels: int = config['n_mels'] # 80\n",
    "n_fft: int = config['n_fft'] # 1024\n",
    "sample_rate: int = config['sample_rate'] # 22050\n",
    "hop_size: int = config['hop_size'] # 256\n",
    "win_size: int = config['win_size'] # 1024\n",
    "f_min: int = config['f_min'] # 0\n",
    "f_max: int = config['f_max'] # 8000\n",
    "random_seed: int = config['random_seed'] # 1234\n",
    "print(n_mels, n_fft, sample_rate, hop_size, win_size, f_min, f_max, random_seed)\n",
    "\n",
    "# for text analysis\n",
    "print(f\"phn2id_path: {config['phn2id_path']}\")\n",
    "with open(config['phn2id_path']) as f:\n",
    "    phn2id = json.load(f)\n",
    "\n",
    "vocab_size = len(phn2id) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0d3c72d0-ef12-4612-bf6f-154ef2cd2d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hifigan_randomseed: 1234\n"
     ]
    }
   ],
   "source": [
    "# for hifigan\n",
    "# setting file paths\n",
    "# from https://github.com/huawei-noah/Speech-Backbones/tree/main/Grad-TTS/hifi-gan\n",
    "# https://drive.google.com/drive/folders/1-eEYTB5Av9jNql0WGBlRoi-WH2J7bp5Y?usp=sharing\n",
    "HiFiGAN_CONFIG = './hifigan/official_pretrained/LJ_V2/config.json'\n",
    "HiFiGAN_ckpt = './hifigan/official_pretrained/LJ_V2/generator_v2'\n",
    "\n",
    "from hifigan import models, env\n",
    "\n",
    "with open(HiFiGAN_CONFIG) as f:\n",
    "    hifigan_hparams = env.AttrDict(json.load(f))\n",
    "\n",
    "hifigan_randomseed = hifigan_hparams.seed\n",
    "print(f'hifigan_randomseed: {hifigan_randomseed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "bc3523a1-5f4f-4d1b-8460-1f79cbc4611c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all cpu at using device: 52\n",
      "Number of available CPU: 4\n",
      "device: cpu\n",
      "device: 1234\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# for cpu device\n",
    "import os\n",
    "\n",
    "print(f\"all cpu at using device: {os.cpu_count()}\")\n",
    "print(f\"Number of available CPU: {len(os.sched_getaffinity(0))}\") # Number of available CPUs can also be obtained. ,use systemcall at linux.\n",
    "#print(f\"GPU_name: {torch.cuda.get_device_name()}\\nGPU avail: {torch.cuda.is_available()}\\n\")\n",
    "DEVICE = 'cpu'\n",
    "device = torch.device(DEVICE)\n",
    "print(f'device: {device}')\n",
    "\n",
    "# setting random_seed ==============\n",
    "print(f'device: {random_seed}')\n",
    "toybox.set_seed(random_seed)\n",
    "print(str(torch.get_default_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e8841145-bcd3-4961-ad24-1edf9d8ba2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: gradtfk5tts\n",
      "[seq] loading Model\n",
      "model_step: 100\n",
      "model_temp: 1.5\n",
      "loading  logs4model/gradtfk5tts/run_tfk_k5/ckpt/gradtfk5tts_820_651082.pt\n",
      "[seq] Initializing diffusion-TTS...\n",
      "Number of encoder + duration predictor parameters: 3.549137m\n",
      "Number of decoder parameters: 2.044639m\n",
      "Total parameters: 5.593776m\n",
      "[seq] loading HiFiGAN\n",
      "Removing weight norm...\n",
      "loading UTMOS ===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /work/sora-sa/.cache/torch/hub/tarepan_SpeechMOS_v1.2.0\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "from gradtts import GradTTS\n",
    "from gradseptts import GradSepTTS\n",
    "from gradtfktts import GradTFKTTS\n",
    "from gradtfk5tts import GradTFKTTS as GradTFK5TTS\n",
    "from gradtimektts import GradTimeKTTS\n",
    "from gradfreqktts import GradFreqKTTS\n",
    "from gradtfkfultts import GradTFKFULTTS\n",
    "\n",
    "print(f'model_name: {model_name}')\n",
    "print(\"[seq] loading Model\")\n",
    "print(f'model_step: {N_STEP}')\n",
    "print(f'model_temp: {TEMP}')\n",
    "\n",
    "print('loading ', ckpt_path)\n",
    "_, _, state_dict = torch.load(ckpt_path,\n",
    "                            map_location=device)\n",
    "\n",
    "\n",
    "print(\"[seq] Initializing diffusion-TTS...\")\n",
    "if model_name == \"gradtts\":\n",
    "    model = GradTTS.build_model(config4model, vocab_size)\n",
    "elif model_name == \"gradseptts\":\n",
    "    model = GradSepTTS.build_model(config4model, vocab_size)\n",
    "elif model_name == \"gradtfktts\":\n",
    "    model = GradTFKTTS.build_model(config4model, vocab_size)\n",
    "elif model_name == \"gradtfk5tts\":\n",
    "    model = GradTFK5TTS.build_model(config4model, vocab_size)\n",
    "elif model_name == \"gradtfkfultts\":\n",
    "    model = GradTFKFULTTS.build_model(config4model, vocab_size)\n",
    "elif model_name == \"gradtimektts\":\n",
    "    model = GradTimeKTTS.build_model(config4model, vocab_size)\n",
    "elif model_name == \"gradfreqktts\":\n",
    "    model = GradFreqKTTS.build_model(config4model, vocab_size)\n",
    "else:\n",
    "    raise ValueError(f\"Error: '{model_name}' is not supported\")\n",
    "\n",
    "model = model.to(device)\n",
    "model.load_state_dict(state_dict)\n",
    "print(f'Number of encoder + duration predictor parameters: {model.encoder.nparams/1e6}m')\n",
    "print(f'Number of decoder parameters: {model.decoder.nparams/1e6}m')\n",
    "print(f'Total parameters: {model.nparams/1e6}m')\n",
    "\n",
    "# generator ===================\n",
    "print(\"[seq] loading HiFiGAN\")\n",
    "vocoder = models.Generator(hifigan_hparams)\n",
    "\n",
    "vocoder.load_state_dict(torch.load(\n",
    "    HiFiGAN_ckpt, map_location=device)['generator'])\n",
    "vocoder = vocoder.eval().to(device)\n",
    "vocoder.remove_weight_norm()\n",
    "\n",
    "print(\"loading UTMOS ===================================\")\n",
    "predictor_utmos = torch.hub.load(\"tarepan/SpeechMOS:v1.2.0\", \"utmos22_strong\", trust_repo=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0c3b6c4a-fddf-49b0-8c2e-52f4f95a5700",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STEP = 100\n",
    "TEMP = 1.5\n",
    "\n",
    "# temp infer\n",
    "i = 56\n",
    "infer_data_num = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "181f6c70-3e53-45a2-98f6-1bb520b01845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                       | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n",
      "[seq]text2mel\n",
      "phonemes_len: 1016\n",
      "phnid_len: 193\n",
      "gradtfk5tts dt: 26.09882480185479\n",
      "gradtfk5tts RTF: 3.2252011235703133\n",
      "[seq]mel2wav\n",
      "[seq]wav2utmos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████████                                                                            | 1/5 [00:32<02:09, 32.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utmos: 3.861962080001831\n",
      "56\n",
      "[seq]text2mel\n",
      "phonemes_len: 1016\n",
      "phnid_len: 193\n",
      "gradtfk5tts dt: 28.3867592420429\n",
      "gradtfk5tts RTF: 3.4929020161107474\n",
      "[seq]mel2wav\n",
      "[seq]wav2utmos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|██████████████████████████████████████                                                         | 2/5 [01:01<01:32, 30.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utmos: 3.9090278148651123\n",
      "56\n",
      "[seq]text2mel\n",
      "phonemes_len: 1016\n",
      "phnid_len: 193\n",
      "gradtfk5tts dt: 27.371107167564332\n",
      "gradtfk5tts RTF: 3.2473146578253784\n",
      "[seq]mel2wav\n",
      "[seq]wav2utmos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████████████                                      | 3/5 [01:35<01:03, 31.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utmos: 3.5535855293273926\n",
      "56\n",
      "[seq]text2mel\n",
      "phonemes_len: 1016\n",
      "phnid_len: 193\n",
      "gradtfk5tts dt: 28.324207940138876\n",
      "gradtfk5tts RTF: 3.4409643042581\n",
      "[seq]mel2wav\n",
      "[seq]wav2utmos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████████████████████████████████████████████████                   | 4/5 [02:10<00:33, 33.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utmos: 4.15135383605957\n",
      "56\n",
      "[seq]text2mel\n",
      "phonemes_len: 1016\n",
      "phnid_len: 193\n",
      "gradtfk5tts dt: 28.249231291934848\n",
      "gradtfk5tts RTF: 3.4464387282398823\n",
      "[seq]mel2wav\n",
      "[seq]wav2utmos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [02:44<00:00, 32.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utmos: 4.078372001647949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in tqdm(range(infer_data_num)):\n",
    "    # temp infer\n",
    "    i = 56\n",
    "    print(i)\n",
    "    print('[seq]text2mel')\n",
    "    text = test_ds_list[i]['text']\n",
    "    phonemes, phnid = text2phnid(text, phn2id, 'en')\n",
    "    phonemes_len_int = len(phonemes)\n",
    "    phnid_len_int = len(phnid)\n",
    "    print(f'phonemes_len: {phonemes_len_int}')\n",
    "    print(f'phnid_len: {phnid_len_int}')\n",
    "    phnid_len = torch.tensor(len(phnid), dtype=torch.long).unsqueeze(0).to(device)\n",
    "    phnid = torch.tensor(phnid).unsqueeze(0).to(device)\n",
    "\n",
    "    # [seq] synth speech\n",
    "    # process text to mel\n",
    "    # mel is [n_mels, n_frame]\n",
    "    start_time = time.perf_counter()\n",
    "    _, mel_prediction, _ = model.forward(phnid,\n",
    "                                     phnid_len,\n",
    "                                     n_timesteps=N_STEP,\n",
    "                                     temperature=TEMP,\n",
    "                                     solver='original')\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    dt = end_time - start_time\n",
    "    dt4mel = dt * 22050 / ( mel_prediction.shape[-1] * 256)\n",
    "    print(f'{model_name} dt: {dt}')\n",
    "    print(f'{model_name} RTF: {dt4mel}')\n",
    "    \n",
    "    # for save mel\n",
    "    x = mel_prediction.unsqueeze(0) # [batch, channel(freq), n_frame(time)] ex.[1, 80, 619]\n",
    "    \"\"\"\n",
    "    # save\n",
    "    #mel_npy_path =  RESULT_MEL_DIR_PATH / f\"{test_ds_filename}.npy\"\n",
    "    #print(f'test_ds_index_{i}: {mel_npy_path}')\n",
    "    np.save(mel_npy_path, mel4save.cpu().detach().numpy().copy())\n",
    "    \"\"\"\n",
    "\n",
    "    # [seq]mel2wav =========================================================\n",
    "    print('[seq]mel2wav')\n",
    "    \"\"\"\n",
    "    x = np.load(mel_npy_path) # [1, n_mel, n_frame]\n",
    "    \"\"\"\n",
    "    x2audio = torch.FloatTensor(x).to(device)\n",
    "    x2audio = x2audio.squeeze().unsqueeze(0)\n",
    "    # x2audio is [1, n_mels, n_frames]\n",
    "    assert x2audio.shape[0] == 1\n",
    "    with torch.no_grad():\n",
    "        # vocoder.forward(x).cpu() is torch.Size([1, 1, 167168])\n",
    "        audio = (vocoder.forward(x2audio).cpu().squeeze().clamp(-1,1).numpy() * 32768).astype(np.int16)\n",
    "    \"\"\"\n",
    "    write(\n",
    "        synth_wav_path,\n",
    "        hifigan_hparams.sampling_rate,\n",
    "        audio)\n",
    "    \"\"\"\n",
    "    # [seq]wav2utmos =========================================================\n",
    "    print('[seq]wav2utmos')\n",
    "    #iwav_path = RESULT_WAV_DIR_PATH / f\"{filename}.wav\"\n",
    "    #wav, samplerate = torchaudio.load(iwav_path)\n",
    "    \"\"\"\n",
    "    wav, samplerate = torchaudio.load(synth_wav_path)\n",
    "    \"\"\"\n",
    "    audio = torch.from_numpy(audio).unsqueeze(0).to(torch.float32)\n",
    "    score_utmos = predictor_utmos(audio, 22050)\n",
    "    score_utmos_float = score_utmos.item()\n",
    "    print(f'utmos: {score_utmos_float}')\n",
    "    #eval_dict = {'name': filename, 'path': str(iwav_path), 'utmos': score_float}\n",
    "    #score_utmos_list.append(eval_dict)\n",
    "    \n",
    "    # path, テキスト文、phonimes, phonimes数, dt, RTF, utmos\n",
    "    \"\"\"\n",
    "    eval_dict = {\n",
    "        'name': test_ds_filename,\n",
    "        'phonemes_len': phonemes_len_int,\n",
    "        'phnid_len': phnid_len_int,\n",
    "        'dt': dt,\n",
    "        'RTF4mel': dt4mel,\n",
    "        'utmos': score_utmos_float\n",
    "    }\n",
    "    eval_list.append(eval_dict)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "10611b61-92cb-400b-bce9-0254b393631c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradtfk5tts\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(model_name)\n",
    "print(N_STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4028f76a-6913-4f27-a9d8-935aebf07aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.23,4.20,4.35,4.24\n",
    "#3.664,4.26,4.04,3.889\n",
    "#4.34, 4.31,4.33, 4.163\n",
    "#3.95, 3.923,4.178, 3.765"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83295ca6-ec60-4550-9b4b-2d75031c3a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfk\n",
    "#N=50:3.8072, 3.899, 3.64, 3.597, 3.504\n",
    "#N=100:3.697, 4.347, 3.898, 3.750, 4.1905\n",
    "#N=200:3.938, 4.109, 3.854, 3.869, 3.709\n",
    "#tfk5\n",
    "#N=50:3.880,3.738, 3.849, 3.290, 3.657\n",
    "#N=100:3.861, 3.9090, 3.5535, 4.151, 4.078"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3921c6a3-1474-4cfd-b1b3-4ec981b8ae68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4770"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "219419-214649"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f3c55-74d7-47b4-a983-340426a43112",
   "metadata": {},
   "outputs": [],
   "source": [
    "30000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
