{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6428c63-a511-4597-bf33-0c0735cc0c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import datetime as dt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from gradtts import GradTTS\n",
    "\n",
    "\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2004f08e-69f9-432f-9abf-14e9837ee396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_phn_to_id(phonemes, phn2id):\n",
    "    \"\"\"\n",
    "    phonemes: phonemes separated by ' '\n",
    "    phn2id: phn2id dict\n",
    "    \"\"\"\n",
    "    return [phn2id[x] for x in ['<bos>'] + phonemes.split(' ') + ['<eos>']]\n",
    "\n",
    "def text2phnid(text, phn2id, language='en', add_blank=True):\n",
    "    if language=='en':\n",
    "        from text import G2pEn\n",
    "        word2phn = G2pEn()\n",
    "        phonemes = word2phn(text)\n",
    "        if add_blank:\n",
    "            phonemes = ' <blank> '.join(phonemes)\n",
    "        return phonemes, convert_phn_to_id(phonemes, phn2id)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Language should be en (for English)!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb419c5-bea7-4f1c-9b06-4ebe5f6b1f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mel(tensors, titles):\n",
    "    xlim = max([t.shape[1] for t in tensors])\n",
    "    fig, axs = plt.subplots(nrows=len(tensors),\n",
    "                            ncols=1,\n",
    "                            figsize=(12, 9),\n",
    "                            constrained_layout=True)\n",
    "    for i in range(len(tensors)):\n",
    "        im = axs[i].imshow(tensors[i],\n",
    "                           aspect=\"auto\",\n",
    "                           origin=\"lower\",\n",
    "                           interpolation='none')\n",
    "        plt.colorbar(im, ax=axs[i])\n",
    "        axs[i].set_title(titles[i])\n",
    "        axs[i].set_xlim([0, xlim])\n",
    "    fig.canvas.draw()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f3ce42-7c63-4319-bc82-08ed73e01b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup HiFi-GAN\n",
    "\n",
    "from hifigan import models, env\n",
    "\n",
    "HiFiGAN_CONFIG = ''\n",
    "HiFiGAN_ckpt = ''\n",
    "with open(HiFiGAN_CONFIG) as f:\n",
    "    hifigan_hparams = env.AttrDict(json.load(f))\n",
    "\n",
    "generator = models.Generator(hifigan_hparams)\n",
    "\n",
    "generator.load_state_dict(torch.load(\n",
    "    HiFiGAN_ckpt, map_location='cpu')['generator'])\n",
    "generator = generator.eval()\n",
    "generator.remove_weight_norm()\n",
    "\n",
    "\n",
    "def convert_mel_to_audio(mel):\n",
    "    # only support batch size of 1\n",
    "    assert mel.shape[0] == 1\n",
    "    with torch.no_grad():\n",
    "        audio = generator(mel).squeeze(1)  # (b,t)\n",
    "    return audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b4cb3d-7b34-42d1-a547-caa4d1f923a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference for ljspeech\n",
    "\n",
    "N_SPKS = 1\n",
    "N_STEP = 50 #4\n",
    "TEMP = 1.3 #1.5\n",
    "STREAMING_CLIP_SIZE = 0.5  # in seconds\n",
    "\n",
    "config_path = 'config/ljspeech_config.yaml'\n",
    "ckpt_path = ''\n",
    "\n",
    "print('loading ', ckpt_path)\n",
    "_, _, state_dict = torch.load(ckpt_path,\n",
    "                              map_location='cpu')\n",
    "\n",
    "\n",
    "with open(config_path) as f:\n",
    "    config = yaml.load(f, yaml.SafeLoader)\n",
    "\n",
    "with open(config['phn2id_path']) as f:\n",
    "    phn2id = json.load(f)\n",
    "vocab_size = len(phn2id) + 1\n",
    "\n",
    "model = GradTTS.build_model(config, vocab_size)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f250eb-41c2-4a85-93d0-8d3f7eb1a673",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a test\"\n",
    "\n",
    "phonemes, phnid = text2phnid(text, phn2id, 'en')\n",
    "print(f'phoneme seq: {phonemes}', type(phonemes))\n",
    "phnid_len = torch.tensor(len(phnid), dtype=torch.long).unsqueeze(0)\n",
    "phnid = torch.tensor(phnid).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893dd67f-e3fc-4dd3-a4d4-909f0fc93248",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "N_SPKS = 1  # 247 for Libri-TTS model and 1 for single speaker (LJSpeech)\n",
    "y_enc, y_dec, attn = generator.forward(x, x_lengths, n_timesteps=50, temperature=1.3,\n",
    "                                       stoc=False, spk=None if N_SPKS==1 else torch.LongTensor([15]).cuda(),\n",
    "                                       length_scale=0.91)\n",
    "\"\"\"\n",
    "\n",
    "y_enc, y_dec, attn = model.forward(phnid,\n",
    "                                   phnid_len,\n",
    "                                   n_timesteps=N_STEP,\n",
    "                                   temperature=TEMP,\n",
    "                                   solver='original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70fbc33-d26f-4f3e-a40e-2c1fd881687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ydec[0]\n",
    "ipd.display(ipd.Audio(convert_mel_to_audio(mel_prediction), rate=22050))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
